If you assume those lessons were all about learning how to write good code, these next few lessons are going to be about training yourself to figure out the best code to write – the most elegant solution to the problem at hand. It becomes particularly important whenever you start working with large data sets, like when your website becomes highly successful.

# Algorithms:
A set of instructions for solving some problem step by step. Algorithms are everywhhere. 
Algorithms become important in web development when you have a lot of data that you have to process or when you are performing complicated transformations on that data. For example, consider the Quora search engine. You could probably develop a similar search engine without much algorithms experience. However, once your site gained thousands or millions of users, your search engine would not be able to handle the large bandwidth of requests. 
Algorithms let you figure out efficient ways to handle large amounts of data.

Pseudocode is an English way to state an algorithm. 

# Recursion:
Recursion is the idea that a function calls itself. It is used to take on bigger problems and break them down into smaller pieces. Think ## Divide and Conquer ##. 
` In computer science, divide and conquer (D&C) is an important algorithm design paradigm based on multi-branched recursion. A divide and conquer algorithm works by recursively breaking down a problem into two or more sub-problems of the same (or related) type, until these become simple enough to be solved directly. The solutions to the sub-problems are then combined to give a solution to the original problem. `

The fact is, any problem you can solve recursively, you can also solve using the iterators that you know and love. If you find yourself saying “why didn’t I just use a while loop here?” then you probably should have. Some problems also break down into far too many pieces and totally overwhelm your computer’s memory. There’s a balance.

Recursion is a programming pattern that is useful in situations when a task can be naturally split into several tasks of the same kind, but simpler. Or when a task can be simplified into an easy action plus a simpler variant of the same task. Or, as we’ll see soon, to deal with certain data structures. The basis of recursion is function arguments that make the task so simple that the function does not make further calls.
Example: A power function
pow(2, 2) = 4
pow(2, 3) = 8
pow(2, 4) = 16 

function pow(x, n) { // for loop approach
  let result = 1;
  // multiply result by x n times in the loop
  for (let i = 0; i < n; i++) {
    result *= x;
  }
  return result;
}
alert( pow(2, 3) ); // 8

function pow(x, n) { // RECURSIVE APPROACH
  if (n == 1) {
    return x;
  } else {
    return x * pow(x, n - 1);
  }
}
alert( pow(2, 3) ); // 8
AKA
              if n==1  = x
             /
pow(x, n) =
             \
              else     = x * pow(x, n - 1)
1. If n == 1, then everything is trivial. It is called the base of recursion, because it immediately produces the obvious result: pow(x, 1) equals x.
2. Otherwise, we can represent pow(x, n) as x * pow(x, n - 1). In maths, one would write xn = x * xn-1. This is called a recursive step: we transform the task into a simpler action (multiplication by x) and a simpler call of the same task (pow with lower n). Next steps simplify it further and further until n reaches 1.
3. We can also say that pow recursively calls itself till n == 1.
4. For example, to calculate pow(2, 4) the recursive variant does these steps:
pow(2, 4) = 2 * pow(2, 3)
pow(2, 3) = 2 * pow(2, 2)
pow(2, 2) = 2 * pow(2, 1)
pow(2, 1) = 2

It gets simpler and simpler until it gets obvious. Recursion is also usually SHORTER than iterative approaches. 

## Defitiion: RECURSION DEPTH - The maximal number of nested calls (including the first one) 
In this function it will be n. The maximal recursion depth is limited by JavaScript engine. We can rely on it being 10,000, some engines allow more, but 100000 is probably out of limit for the majority of them. There are automatic optimizations that help alleviate this. 

The information about the process of execution of a running function is stored in its "execution context".
Execution context is an internal data structure that contains details about the execution of a function: the control flow, variables, the value of this and some other data. 1 function = 1 execution context. 

When a function makes a nested call, the following happens:
The current function is paused.
The execution context associated with it is remembered in a special data structure called execution context stack. The current context is “remembered” on top of the stack.
The new context is created for the subcall.
The nested call executes.
After it ends, when the subcall is finished – the previous context is popped from the stack, and its execution continues. The old execution context is retrieved from the stack, and the outer function is resumed from where it stopped.

Here’s the context stack when we entered the subcall pow(2, 1):
Context: { x: 2, n: 1, at line 1 } pow(2, 1) - subcall first
Context: { x: 2, n: 2, at line 1 } pow(2, 2) - subcall
Context: { x: 2, n: 3, at line 5 } pow(2, 3) -  first call

The recursion depth in this case was: 3.

### Any recursion can be rewritten as a loop. The loop variant usually can be made more effective.

## Example:
`let company = {
  sales: [{
    name: 'John',
    salary: 1000
  }, {
    name: 'Alice',
    salary: 1600
  }],

  development: {
    sites: [{
      name: 'Peter',
      salary: 2000
    }, {
      name: 'Alex',
      salary: 1800
    }],

    internals: [{
      name: 'Jack',
      salary: 1300
    }]
  }
};`
It is also possible that when a subdepartment grows, it divides into subsubdepartments (or teams). For instance, the sites department in the future may be split into teams for siteA and siteB. And they, potentially, can split even more. That’s not on the picture, just something to have in mind.

### Let’s try recursion. ### 
As we can see, when our function gets a department to sum, there are two possible cases:
Either it’s a “simple” department with an array of people – then we can sum the salaries in a simple loop.
Or it’s an object with N subdepartments – then we can make N recursive calls to get the sum for each of the subdeps and combine the results.
1. The 1st case is the base of recursion, the trivial case, when we get an array.
2. The 2nd case when we get an object is the recursive step. A complex task is split into subtasks for smaller departments. They may in turn split again, but sooner or later the split will finish at (1).

let company = { // the same object, compressed for brevity
  sales: [{name: 'John', salary: 1000}, {name: 'Alice', salary: 1600 }],
  development: {
    sites: [{name: 'Peter', salary: 2000}, {name: 'Alex', salary: 1800 }],
    internals: [{name: 'Jack', salary: 1300}]
  }
};
// The function to do the job
function sumSalaries(department) {
  if (Array.isArray(department)) { // case (1)
    return department.reduce((prev, current) => prev + current.salary, 0); // sum the array
  } else { // case (2)
    let sum = 0;
    for (let subdep of Object.values(department)) {
      sum += sumSalaries(subdep); // recursively call for subdepartments, sum the results
    }
    return sum;
  }
}
alert(sumSalaries(company)); // 7700

We can easily see the principle: for an object {...} subcalls are made, while arrays [...] are the “leaves” of the recursion tree, they give immediate result.

## Linked List: 
There’s a big problem with arrays... The “delete element” and “insert element” operations are expensive. 
For instance, arr.unshift(obj) operation has to renumber all elements to make room for a new obj, and if the array is big, it takes time. Same with arr.shift().
If we really need fast insertion/deletion, we can choose another data structure called a LINKED LIST.

## The linked list element is recursively defined as an object with:
- value.
- "next" property referencing the next linked list element or null if that’s the end.
For instance:
let list = {
  value: 1,
  next: {
    value: 2,
    next: {
      value: 3,
      next: {
        value: 4,
        next: null
      }
    }
  }
};

A linked list is a linear collection of data elements called nodes that “point” to the next node by means of a pointer.
Each node holds a single element of data and a link or pointer to the next node in the list.
A head node is the first node in the list, a tail node is the last node in the list. Below is a basic representation of a linked list:
* [ NODE(head) ] -> [ NODE ] -> [ NODE(tail) ] -> null

## A recursively-defined data structure is a data structure that can be defined using itself.
list = { value, next -> list }

### Alternative for creation: 
let list = { value: 1 };
list.next = { value: 2 };
list.next.next = { value: 3 };
list.next.next.next = { value: 4 };
list.next.next.next.next = null;

## Another example: 
`let secondList = list.next.next;`
`list.next.next = null;`
- Breaking and Rejoining a List: The sequence of operations demonstrates how you might temporarily break a linked list into two parts and then rejoin them. This could be useful in certain algorithms where a temporary separation of the list is needed, such as reversing a portion of the list, splitting it for parallel processing, or reorganizing nodes.

- Intermediate State: When you set list.next.next = null, the list is temporarily split into two separate lists. One list starts with list and ends at list.next, and the other list starts with secondList.

`list.next.next = secondList;`

- Final State: By setting list.next.next = secondList, you restore the original link, resulting in a single continuous list as it was initially. This pattern is common in linked list operations, where intermediate states are used to facilitate complex manipulations like sorting, merging, or rearranging nodes.


let list = { value: 1 };
list.next = { value: 2 };
list.next.next = { value: 3 };
list.next.next.next = { value: 4 };

// prepend the new value to the list
list = { value: "new item", next: list };

* To remove a value from the middle, change next of the previous one:
list.next = list.next.next;

We made list.next jump over 1 to value 2. The value 1 is now excluded from the chain. If it’s not stored anywhere else, it will be automatically removed from the memory. Unlike arrays, there’s no mass-renumbering, we can easily rearrange elements.
* The main drawback is that we can’t easily access an element by its number. In an array that’s easy: arr[n] is a direct reference. But in the list we need to start from the first item and go next N times to get the Nth element.

Singly linked lists - only flow from the head node to the tail
Doubly linked lists - can go forward or backward bec ause each node holds a reference to the next and previous node

## Lists can be enhanced:
- We can add property prev in addition to next to reference the previous element, to move back easily.
- We can also add a variable named tail referencing the last element of the list (and update it when adding/removing elements from the end).
- The data structure may vary according to our needs.

SLICK
function fib(n) {
  let a = 1;
  let b = 1;
  for (let i = 3; i <= n; i++) {
    let c = a + b;
    a = b;
    b = c;
  }
  return b;
}

alert( fib(3) ); // 2
alert( fib(7) ); // 13
alert( fib(77) ); // 5527939700884757


printList(list) {
    if (list.next === null) {
        console.log(list.value) 
    } else {
        console.log(list.value);
        printList(list.next); 
    }
}

reverse a linked list

let list = {
  value: 1,
  next: {
    value: 2,
    next: {
      value: 3,
      next: {
        value: 4,
        next: null
      }
    }
  }
};

function printReverseList(list) {
  if (list.next) {
    printReverseList(list.next);
  }
  alert(list.value);
}
printReverseList(list);


function sumRangeRecursive(n, total = 0) {
    if (n <= 0) {
        return total
    }
    return sumRangeRecursive(n - 1, total + n) 
}
what this does:
sumRangeRecoursive(3, 0)
    sumRangeRecoursive(2, 3)
        sumRangeRecoursive(1, 5)
            sumRangeRecoursive(0, 6)
            return 6
        return 6
    return 6
return 6

The 5 steps to solve any recursive problem:
1. Find the simplest case (base case)
2. Play around with examples and visualize.
3. Relate the harder cases to the simpler cases.
4. Generalize pattern.
5. Combine recursive pattern with base case using code.

function collatzConjecture(n, step = 0) {
    if (n = 1) {
        return step
    } else if (n % 2 === 0) {
        return collatzConjecture((n/2), step + 1)
    } else if (n % 2 !== 0) {
        return collatzConjecture((3n + 1), step + 1)
    }
}

^ this is not necessary to keep track of the step because each recursive situation only adds 1 to the step count

function collatzConjecture(n, step = 0) {
    if (n = 1) {
        return step
    } else if (n % 2 === 0) {
        return 1 + collatzConjecture((n/2))
    } else if (n % 2 !== 0) {
        return 1 + collatzConjecture((3n + 1))
    }
}

# 5 STEPS TO SOLVE ANY RECURSIVE PROBLEM
 - 1. What is the simplest possible input?
 - 2. Play around with examples and visualize. 
 - 3. Relate harder cases to simple cases. 
 - 4. Generalize the pattern
 - 5. Write code by combining the recursive pattern with the base case. 


The first insight is that iteration is a special case of recursion.
        void do_loop () { do { ... } while (e); }
is equivalent to:
        void do_loop () { ... ; if (e) do_loop(); }


* int sumTailRecursive(int n, int accu = 0) { // tail recursive and 
    if (n == 0) return accu;
    return sumTailRecursive(n-1, accu + n);
}
Here, the recursive call sumTailRecursive(n-1, accu + n) is the last operation, making it tail recursive. A compiler can optimize this function to run in constant space.


# MERGE SORT #
 - The main idea is to sort smaller arrays and combine those arrays into the correct order. 
 - the most complicated type of sort in CS50
 - Sort the left, sort the right, merge the sorted halves together. 
 Merge Sort is just these 3 steps recursively. Dividing and Conquering

 Single elements are sorted by default. What does it mean to merge sorted halves?
 take this example:
 2457 0136
 Compare 2 to 0: which number is less?
 0... now compare 2 to 1? 
 01
 now compare 2 to 3? 
 012
 now compare 4 and 3?
 0123 
 4 and 6?
 01234
 5 and 6?
 012345
 7 and 6?
 0123456
 only 7 left? 01234567

# QUICK NOTE ON LOGARITHMS #
 Logarithms:
bn = p

 p is the logarithm of n
 log  n = p  ==> b ^ p = n OR b raised to what power = n 
    `b
 log  8 = 3
    `2


# TIME COMPLEXITY #  
In programming, there are two ways we can measure the efficiency of our code. We can measure the time complexity or the space complexity. It’s important to understand that you never measure the efficiency of an algorithm by how long it takes to execute.

The way to measure code efficiency is to evaluate how many ‘steps’ it takes to complete. A 5 step program will always run faster than a 20 step program. Steps are key. 

function oddNumbersLessThanTen() {
  let currentNumber = 1;
  while (currentNumber < 10) {
    if (currentNumber % 2 !== 0) {
      console.log(currentNumber);
    }
    currentNumber += 1;
  }
}

So there are 3 steps for every loop iteration and it iterates 9 times which is 27 steps. Then we have one step which iterates for only half the loop iteration which is 5 steps. Assigning an initial value to currentNumber and checking the exit condition of the loop is one step each. 27 + 5 + 1 + 1 = 34 steps. But for other functions and algorithms it depends on a lot of factors to get to the final answer of number of steps. 

- Asymptotic Notations:
Asymptotic Notations are used to describe the running time of an algorithm. Because an algorithm’s running time can differ depending on the input, there are several notations that measure that running time in different ways. The 3 most common are as follows:

-- Big O Notation * - represents the upper bound of an algorithm. This means the worst-case scenario for how the algorithm will perform. Big O is most common because as applications scale, you need to be sure of the worse case scaenario. Big O only wants to tell us an algorithm’s complexity relative to the size of the input.
The Big O Notations in the order of speed from fastest to slowest are:

- O(1) - Constant Complexity: array[2] - no matter how big an array gets, accessing an element is constant 1 step - this is as good as it gets for time complexity; 
You may have thought a moment ago, is it really just one step? The answer is technically no, in reality the computer must first look up where the array is in memory, then from the first element in the array it needs to jump to the index argument provided. That’s at least a couple of steps. So you wouldn’t be wrong for writing something like O(1 + 2(steps)). However, the 2 steps are merely incidental. With an array of 10,000 elements, it still takes the same amount of steps as if the array was 2 elements. Because of this, Big O doesn’t concern itself with these incidental numbers. They don’t provide any context to how the complexity grows when the data size changes, because they are constant, and so in Big O they are dropped.
- O(log N) - Logarithmic Complexity: the numbers of steps an algorithm takes increases by 1 as the data doubles; This is still pretty efficient (going from 5000 elements to 10000 elements only adds one step); BINARY SEARCH is here
- O(N) - Linear Complexity: as the number of items grows, the number of steps grows at exactly the same rate. This happens whenever you iterate over an array, therefore as the number of elements grows, your steps grow. 
- O(N log N) - N x log N Complexity: This is like splitting a big problem into smaller problems and then doing work on each smaller part; MERGE SORT
- O(n²) - Quadratic Complexity: double nested loops; 
- O(n³) - Cubic Complexity: triple nested loops; 
- O(2ⁿ) - Exponential Complexity: with each item added to the data size, the number of steps doubles from the previous number of steps - you want to avoid this as much as possible because it is very slow - Fibonacci is an example
- O(N!) - Factorial Complexity: if you ever need to calculate permutations or combinations - The factorial of 3 is 6 (3 * 2 * 1). The factorial of 4 is 24. The factorial of 10? 3,628,800. Things get out of hand quickly, avoid at all costs

-- Omega Notation - represents the lower bound of an algorithm. This is the best-case scenario.
Omega Notation isn’t considered as useful because it is unlikely our item will often be the first item in our data structure search, so it doesn’t give us any idea how well the algorithm will scale.

-- Theta Notation - represents both the upper bound and lower bound and therefore analyses the average case complexity of an algorithm. It looks to give the EXACT value or a useful range between narrow upper and lower bounds. 

# SPACE COMPLEXITY #
When we talk about memory, we mean primary memory, which is the working memory available to your system to execute algorithms. Primary memory is a segment of computer memory that can be accessed directly by the processor and is the part of the computer that stores current data, programs, and instructions. . In a hierarchy of memory, primary memory has access time less than secondary memory and greater than cache memory. 

Anyways:
Space complexity can be considered to be the total space used by an algorithm relative to the size of the input. Measuring space complexity considers the space used by your algorithm input and auxiliary space. Auxiliary space is the extra space used by the algorithm. It is the total amount of working memory our algorithm needs. 

The first thing to know is that, like time complexity, we measure space complexity by considering all steps including any constants, and then we drop the constants when applying a Big O Notation to the algorithm.
Most data structures you come across will have a space complexity of O(N). That makes sense - when you increase the number of items in your data structure, it increases the space that data structure occupies in a linear way.

Space complexity is a measure of the amount of working storage an algorithm needs. That means how much memory, in the worst case, is needed at any point in the algorithm. As with time complexity, we're mostly concerned with how the space needs grow, in big-Oh terms, as the size N of the input problem grows.

-- Memoization - It's like the program is taking notes on problems it has already solved. When the program needs to solve the same problem again, it checks its "notes" first. If the answer is there, it uses that instead of recalculating everything from scratch.
This is super useful for problems that:
- Take a long time to solve
- Get asked over and over again

Constant time and space complexity is as good as an algorithm can get, but it’s not always possible.

There is usually a trade-off between space complexity and time complexity: to increase the speed of an algorithm, you’ll likely need to store more variables in memory.

 // TIMER TRICK
const t0 = performance.now()
// Add function here...
const t1 = performance.now()
console.log("The function took: " + (t1 - t0) + " milliseconds.")


# DATA STRUCTURES #
The basic idea of a data structure is to store data in a way that meets the needs of your particular application. Searching algorithms are paramount for large data sets. Two to look at: breadth-first-search and depth-first-search. 
In computer science, a **data structure** is a way to organize and store data so that it can be accessed and used efficiently. Think of it like a special box where you keep your things organized, making it easy to find what you need.

### Key Points:
1. **What is a Data Structure?**  
   - It's a collection of data values, how they relate to each other, and the operations you can perform on them.
2. **Why Use Data Structures?**  
   - Different types of data structures are better for different tasks. For example, some help you find information quickly, while others make it easy to add or remove items.
3. **Common Types of Data Structures:**
   - **Arrays:** A list of items in a specific order. You can think of it like a row of lockers, where each locker has a number.
   - **Linked Lists:** A series of connected items where each item points to the next one. It’s like a chain of paperclips.
   - **Hash Tables:** A way to store data that allows for quick lookups, like a dictionary where you can find a word really fast.
   - **Graphs:** A collection of points (called nodes) connected by lines (called edges), used to show relationships, like a map of friends on social media.
   - **Stacks and Queues:** Special ways to organize items where you can only add or remove from one end (stack) or from both ends (queue).
   - **Trees:** A structure that organizes data in a hierarchy, like a family tree.
4. **How Are They Used?**  
   - Data structures help manage large amounts of information efficiently, like in databases or when searching the internet.
5. **Programming Languages:**  
   - Many programming languages have built-in support for data structures, making it easier for programmers to use them in their code.
In summary, data structures are essential tools in computer science that help organize and manage data effectively, making it easier to perform various tasks with that data.

## ALGORITHMS: Searching Through a Phonebook For A Name

- Linear Search Algorithm: searching names one by one by last name - "Aaron? no. Abel? no. Abraham? no. etc"; Tens of thhousands of comparisons until you find the right name. The process of going start to finish through a list and comparing values. This is brute force. 
- Chunking Search Algorithm: The process of chunking involves first finding the general area where an entry would be, then proceeding to check every entry. This way is suboptimal.
- Binary Search Algorithm: split the phonebook is half and determine which half the entry you're looking for is on, then throw away the other half. Repeat until the entry is found. Divide and conquer; THE DATA MUST BE SORTED FOR THIS TO WORK
log2(400) = 8.64385618977, which means that in the worst case rounding up, it would take 9 comparisons.
What about 4,000,000 pages? log2(4,000,000) = 21.9315685693, which means it would take at most 22 comparisons.

In order to to use BFS or DFS you have to understand and implement certain concepts first. Without them, it's like trying to fix something without the proper tools. 

## Queues
You can find a great example of a queue simply by thinking about a typical experience at a deli. When you get in line, you pull a number and wait for your number to be called. Everyone who pulled a number before you will be given service before you, but when you’re the customer who has been in line the longest, your number will be called next. Queues are FIFO - First In, First Out
Queues have 2 properties:
- Enqueue is when you “start waiting.” It’s the equivalent of pulling a number in the deli example.
- Dequeue is when it’s your turn to be served. It’s the equivalent of having your number called at the deli.

It's NECESSARY to have 2 different pointers in the queue that point to the front of the queue and the back of the queue. The queue is "full" when the 'back' pointer is is directly to the left of the 'front' pointer. We can cycle our pointers throughout our structure by using the modulo operator. So our back pointer can technically be in front of our front pointer. 
Next Pointer position = (pointer + 1) % length of queue/array
Example:
A queue of length of 5. We move the end pointer to the back and then there are no more spaces. 
next pointer position = ([4] + 1) % 5 ==> 5 % 5 ==> 0 so the next position for the pointer is position 0

## Stacks
This is like a stack of books. If you want to add something to thhe stack, it goes on top of the stack. You can only add to the top or remove from the top. Stacks are LIFO - last in, first out. Sorted by insertion order, elements are not indexed. Middle items cannot be accessed directly in stacks of queues

# BINARY SEARCH TREES: 
Binary search trees (BSTs) are a special type of tree data structure where each node has at most two children, referred to as the left and right child. In a BST, for any given node, all elements in its left subtree are smaller than the node, and all elements in its right subtree are larger. This property makes BSTs efficient for searching, inserting, and deleting elements, typically with an average time complexity of O(log n) for these operations.

To construct a BST from an array, you start with an empty tree and insert each element of the array one by one. For each insertion, you compare the new element with the root. If it's smaller, you move to the left child; if larger, you move to the right child. You continue this process until you find an empty spot to insert the new node. Traversing a BST can be done in several ways: in-order (left subtree, root, right subtree), pre-order (root, left subtree, right subtree), or post-order (left subtree, right subtree, root). In-order traversal of a BST yields the elements in sorted order, which is a useful property for many applications. 

## BINARY TREE TRAVERSAL
Whhen working with trees, we usually want to visit each of the nodes in the tree. Tree traversal is the process of visiting each node exactly one time in some order. Visiting the nodes involves reading/processing thhe data in the node. Binary search trees are composed of nodes. Each node has up to two direct children. In what order should we visit the nodes? We can either go breadth first or depth first. 
Breadth first - visit all the nodes on the same level before continuing on to other levels. The root is level 0. This is also called to level order traversal - we traverse level by level. Here we visit all the child nodes before the grandchildren. 
Depth first - visit all the child and grandchild nodes before going to the next child and grandchild set. AKA visit the left node and then all the children and grandchildren of the left node, then go on to the right node. There can be differences in how we visit the left subtree and thhe right subtree:
Left, Root, Right OR Right, Root, Left OR ...
3 popular strategies:
Preorder - Root, Left, Right - with the left and right subtrees visited recursevily - DLR
InOrder - Left, Root, Right - "" - LDR
PostOrder - Left, Right, Root - "" - LRD
The names are based on the position of the root. Root = Data AKA D, Left = L, Right = R;
Once a subtree is done being traversed we can read the data. So for example, for InOrder traversal we go all the way down to the left most node, then once the left traversal is done, we can read the data. We always start at the root and then follow the order until we cannot go further.

By convention, the left subtree is always visited before the right subtree
### BREADTH FIRST TRAVERSAL
This is also referred to as level order traversal. We start at the root AKA level 0 and then onto level 1, and then level 2 and so on. However, theres a slight problem - we cannot only use one pointer or variable to keep track of where we should go next because in binary search trees each node is connected to up to 2 nodes and there may not be a way to get to that node directly if they sre not connected. We cannot even go back to the root node because there is no backward link to get back to the root node. How can we traverse the nodes in Breadth First Order?
As we visit a node, we can keep a reference of all of its children in a QUEUE so that we can visit them later. A node that is in the queue can be called a discovered node because we know it's location but we have not visited it yet. Basicall;yy the algorithm is as follows:
- Visit the first node
- Enqueue is two children
- Take out the next node from the queue
- Visit that node
- Enqueue its children
- Etc. 

Queues are FIFO so the order will be preserved. 
The Time Complexity - O(n)
The Space Complexity - O(n) is the worst case and the average case
### DEPTH FIRST TRAVERSAL - PreOrder, InOrder, PostOrder
If we go in any direction in DFS, we visit the complete subtree in that direction. The subtrees are visited in the same order
as all the nodes in the original  subtree. How do we write this functionally?
Node are like objects that have 3 properties: data, address/location of the left node, address/location of the right node
PreOrder:
BASE CASE: if the root is null, return
- Read data in the root node
- Make recursive call to the left subtree
- Make recursive call to the right subtree
InOrder: inorder traversal of a binary search tree gives us the nodes in sorted order
BASE CASE: if the root is null, return
- Make Recursive call to the left subtree
- Read the data in the root node
- Make recursive call to the right subtree
PostOrder:
BASE CASE: if the root is null, return
- Make Recursive call to the left subtree
- Make recursive call to the right subtree
- Read the data in the root node
Time Complexity: O(n)
Space Complexity: O(n) is worst case; O(log n) is the best case

### WHICH SHOULD I USE?###
If you know a solution is not far from the root of the tree, a breadth first search (BFS) might be better.
If the tree is very deep and solutions are rare, depth first search (DFS) might take an extremely long time, but BFS could be faster.
If the tree is very wide, a BFS might need too much memory, so it might be completely impractical.
If solutions are frequent but located deep in the tree, BFS could be impractical.
If the search tree is very deep you will need to restrict the search depth for depth first search (DFS), anyway (for example with iterative deepening). But these are just rules of thumb; you'll probably need to experiment. I think in practice you'll usually not use these algorithms in their pure form anyway. There could be heuristics that help to explore promising parts of the search space first, or you might want to modify your search algorithm to be able to parallelize it efficiently.

Breadth First Search is generally the best approach when the depth of the tree can vary, and you only need to search part of the tree for a solution. For example, finding the shortest path from a starting value to a final value is a good place to use BFS.

Depth First Search is commonly used when you need to search the entire tree. It's easier to implement (using recursion) than BFS, and requires less state: While BFS requires you store the entire 'frontier', DFS only requires you store the list of parent nodes of the current element.

# HASH MAPS / HASH TABLE - DATA STRUCTURES #
- One of the most commonly used data structures
- A hash map takes in a key value pair, produces a hash code, and stores the pair in a bucket.
- Most languages have their own hashing function built in
- Combining an array with a linked list
- Best for non sorted data
- Two parts: hash code and array; Run the data through the hash function and then store the data in the element of the array represented by the hash code

## What is a hash code? What is hashing? ##
Hashing involves taking an input in and generating a corresponding output. A hash function should be a pure function. 
Hashing the same input should ALWAYS return the same hash code, and there should be no random generation component.

function hash(name) { // basic but good nonetheless
  return name.charAt(0);
}

There is a key difference between hashing and ciphering (encryption): reversibility. Hashing is a one-way process. 
If you have a name "Carlos", we can hash it to "C". But it’s impossible to reverse it from "C" back to its original form. You cannot know if it’s "Carlos", maybe it’s "Carla" or "Carrot". We don’t know.

* Hashing is very good for security. Given a password, you can save the hash of that password rather than the password’s plain text. If someone steals your hashes, they cannot know the original passwords since they are unable to reverse the hash back to the password.

function stringToNumber(string) {
  let hashCode = 0;
  for (let i = 0; i < string.length; i++) {
    hashCode += string.charCodeAt(i);
  }
  return hashCode;
}

function hash(name, surname) {
  return stringToNumber(name) + stringToNumber(surname);
}

* Hash codes need to be numbers. Those numbers will serve as the indexes to the buckets we will keep our key value pair in. 
* Rules for choosing good hash function:
1. The hash function should be simple to compute.
2. Number of collisions should be less while placing the record in the hash table. Ideally no collision should occur. Such a function is called perfect hash function.
3. Hash function should produce such keys which will get distributed uniformly over an array.
4. The hash function should depend on every bit of the key. Thus the hash function that simply extracts the portion of a key is not suitable.

TWO METHODS: hashing by division and hashing by multiplication

## BUCKETS ##
- Essentially arrays that we store our elements in
The hash function returns a number that serves as the index of the array at which we store this specific key value pair. Let’s say we wanted to store a person’s full name as a key “Fred” with a value of “Smith”:
- Pass “Fred” into the hash function to get the hash code which is 385.
- Find the bucket at index 385.
- Store the key value pair in that bucket. In this case, the key would be “Fred” and the value would be “Smith”.
What if the bucket at index 385 already contains an item with the same key “Fred”? We check if it’s the same item by comparing the keys, then we overwrite the value with our new value. 

Now if we wanted to get a value using a key:
- To retrieve the value, we hash the key and calculate its bucket number.
- If the bucket is not empty, then we go to that bucket.
- Now we compare if the node’s key is the same key that was used for the retrieval.
- If it is, then we can return the node’s value. Otherwise, we return null.
Maybe you are wondering, WHY are we comparing the keys if we already found the index of that bucket? Remember, A HASH CODE IS JUST A LOCATION. Different keys might generate the same hash code!! We need to make sure the key is the same by comparing both keys that are inside the bucket.
* Insertion order is not maintained: Example: if we insert the values Mao, Zach, Xari in this order, we may get back ["Zach", "Mao", "Xari"] when we call an iterator. If iterating over a hash map frequently is your goal, then this data structure is not the right choice for the job, a simple array would be better.

## COLLISIONS ##
A collision occurs when two different keys generate the exact same hash code. "Sara" and "raSa" will hash to the same location unless we change our hash function. 
function stringToNumber(string) {
  let hashCode = 0;

  const primeNumber = 31;
  for (let i = 0; i < string.length; i++) {
    hashCode = primeNumber * hashCode + string.charCodeAt(i);
  }
  return hashCode;
}
Now because we are multiply the old hashcode with a prime number we will have different locations for each letter and therefore each name. NOTICE the usage of a prime number. We could have chosen any number we wanted, but prime numbers are preferable. Multiplying by a prime number will reduce the likelihood of hash codes being evenly divisible by the bucket length, which helps minimize the occurrence of collisions. We have a FINITE NUMBER OF BUCKETS so we cannot eliminate all colissions, but we can reduce the number of them. 

What if each Node inside the bucket can store more than one value? Enter Linked Lists. Now each bucket would be a linked list. When inserting into a bucket, if it’s empty, we insert the head of Linked List. If a head exists in a bucket, we follow that Linked List to add to the end of it. 

Hash tables / hash maps are arrays with linked lists inside of them. 

## HASH TABLE GROWTH ## 
We don't have infinite buckets. Most programming languages start with the default size of 16 buckets because it’s a power of 2, which helps with some techniques for performance that require bit manipulation for indexes. 
How are we going to insert into those buckets when our hash function generates big numbers like 20353924? We make use of the modulo % operation. 
* Given any number modulo by 16 we will get a number between 0 and 15. For example: "Manon" --> 1091 --> 1091 % 16 --> 3 (bucket #3)

Eventually, however, there will be more nodes than there are buckets, which guarantees a collision. To grow our buckets, we create a new buckets list that is double the size of the old buckets list, then we copy all nodes over to the new buckets. Therefore we need to keep track of how two numbers: CAPACITY and the LOAD FACTOR. 
 - The capacity is the total number of buckets we currently have.
 - The load factor is a number that we assign our hash map to at the start. It’s the factor that will determine when it is a good time to grow our buckets. Hash map implementations across various languages use a load factor between 0.75 and 1.
The product of these numbers is the threshold lets us know when it is time to grow our buckets. 

\\ A hash map has an average case complexity of O(1) for the following methods:
Insertion
Retrieval
Removal
O(n) is the worst case scenario. 

EXAMPLE
class HashMap {
  // ...
  findBucket(key) { // uses murmer3 on the key to find a bucket to use
    let h = murmur3(key);
    return this.bs[h % this.bs.length]; // bs = buckets
  }

judgeEntry(bucket, key) { // determines if a given bucket has an entry with a given key
    for (let e of bucket) {
      if (e.key === key) {
        return e;
      }
    }
    return null;
  }

    set(key, value) { // If an entry is found, its value is overwritten. If no entry is found, the key-value pair is added to the map.
    let b = this.findBucket(key); // chooses a bucket to use
    let e = this.judgeEntry(b, key); // determines in the key is already in the bucket
    if (e) { 
      e.value = value; // if value is there, overwrite
      return;
    }
    b.push({ key, value }); // if no value in the bucket
  }

    get(key) { // gets the value from the hashmap
    let b = this.findBucket(key);
    let e = this.judgeEntry(b, key);
    if (e) {
      return e.value;
    }
    return null;
  }
}






